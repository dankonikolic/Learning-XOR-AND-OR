{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn  \n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NN_multilayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_inp, n_hid, n_layer = 1):\n",
    "        super(NN_multilayer, self).__init__()\n",
    "        self.linear = nn.Linear(n_inp, n_hid)\n",
    "        \n",
    "        self.linear_h =  nn.ModuleList([nn.Linear(n_hid, n_hid) for i in range(n_layer-1)])\n",
    "        \n",
    "        #self.linearM = nn.Linear(n_hid, n_hid)\n",
    "        \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(n_hid, 1)  #here train 10 layers instead of 1 and repeat the experiment\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.Sigmoid(self.linear(input))\n",
    "        \n",
    "        for layer in self.linear_h:\n",
    "            x = self.Sigmoid(layer(x))\n",
    "        \n",
    "        ##x = self.Sigmoid(self.linearM(x))\n",
    "\n",
    "        yh = self.linear2(x)\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStop():\n",
    "    def __init__(self, patience):\n",
    "        self.last = 999999999999999999.99999\n",
    "        self.cnt = 0\n",
    "        self.patience = patience\n",
    "    \n",
    "    def item(self, item):\n",
    "        stop = False\n",
    "        if item >= self.last:\n",
    "            #print(item, self.last)\n",
    "            self.cnt += 1\n",
    "            if self.cnt >= self.patience:\n",
    "                stop = True\n",
    "        else:\n",
    "            self.cnt = 0\n",
    "        self.last = item\n",
    "        \n",
    "        return stop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(scores, y):\n",
    "    predictions = [(0, 1)[i > .5] for i in scores] \n",
    "    num_correct =sum(a_ == b_ for a_, b_ in zip(predictions, y)) \n",
    "    accuracy = num_correct / len(y)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "scores = [0.1, 0.2, 0.5, 0.6, 0.8]\n",
    "y = [0, 1, 0, 1, 1]\n",
    "\n",
    "if accuracy(scores, y) == 1.0:\n",
    "    print(\"done\")\n",
    "\n",
    "print(accuracy(scores, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_once(nr_inp, nr_hid, Xs, y):\n",
    "    #xor_network = XOR(nr_inp, nr_hid).cuda()\n",
    "    \n",
    "    xor_network = NN_multilayer(nr_inp, nr_hid, 1).cuda()\n",
    "    \n",
    "    epochs = 80000 \n",
    "    mseloss = nn.MSELoss() \n",
    "    optimizer = torch.optim.Adam(xor_network.parameters(), lr = 0.03) \n",
    "    all_losses = [] \n",
    "    current_loss = 0 \n",
    "    plot_every = 100 \n",
    "\n",
    "    cuda = torch.device('cuda')  \n",
    "\n",
    "    #Xs = torch.tensor([[0.,0], [0,1],[1.,0], [1,1]]).cuda()\n",
    "    #y = torch.tensor([[0.], [1], [1.], [0]]).cuda()\n",
    "    \n",
    "    converged = False\n",
    "    \n",
    "    \n",
    "    patience = 125    \n",
    "    early_stop = EarlyStop(patience)\n",
    "\n",
    "   \n",
    "    for epoch in tqdm(range(epochs)): \n",
    "\n",
    "        # input training example and return the prediction   \n",
    "        yhat = xor_network.forward(Xs)\n",
    "\n",
    "        # calculate MSE loss   \n",
    "        loss = mseloss(yhat, y)\n",
    "\n",
    "        # backpropogate through the loss gradiants   \n",
    "        loss.backward()\n",
    "\n",
    "        # update model weights   \n",
    "        optimizer.step()\n",
    "\n",
    "        # remove current gradients for next iteration   \n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # append to loss   \n",
    "        current_loss += loss  \n",
    "        \n",
    "        if epoch % plot_every == 0:       \n",
    "            all_losses.append(current_loss / plot_every)  \n",
    "            if (current_loss) < 0.00001:\n",
    "                    converged = True\n",
    "                    #print (epoch)\n",
    "                    break\n",
    "            \n",
    "            \n",
    "            scores = xor_network(Xs)\n",
    "            accur = accuracy(scores, y)\n",
    "            \n",
    "            if accur == 1.0:\n",
    "                converged = True\n",
    "                print(\"Perfect accuracy\")\n",
    "                break\n",
    "            \n",
    "            #print(scores, y, accur)\n",
    "            \"\"\"predictions = [(0, 1)[i < .5] for i in scores] \n",
    "            num_correct += (predictions == y).sum()\n",
    "            \n",
    "            \"\"\"\n",
    "            #if test_performance(): \n",
    "            #    pass\n",
    "            \n",
    "            \n",
    "            res = early_stop.item(current_loss)\n",
    "            if res:\n",
    "                print(\"...early stopping\", current_loss.cpu().detach().numpy())\n",
    "                break\n",
    "                \n",
    "            current_loss = 0\n",
    "\n",
    "        # print progress   \n",
    "        #if epoch > 0 and epoch % 500 == 0:     \n",
    "        #    print(f'Epoch: {epoch} completed')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #print(\"Loss: \", current_loss.cpu().detach().numpy())\n",
    "    \n",
    "    \n",
    "    #To compute the number of trainable parameters:\n",
    "    model_parameters = filter(lambda p: p.requires_grad, xor_network.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    #print(\"Pars:\", params)\n",
    "    \n",
    "    \n",
    "    return current_loss.cpu().detach().numpy(), epoch, converged, params, accur.cpu().detach().numpy()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data for generalized XOR  (inverters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverter_data(level):\n",
    "    O = [[]]\n",
    "    I = [[0]]\n",
    "\n",
    "    for i in range(level):\n",
    "\n",
    "        Out = deepcopy(O) \n",
    "        Inp = deepcopy(I)\n",
    "\n",
    "        for elem in O:\n",
    "            Out.append(elem)\n",
    "\n",
    "        for elem in I:\n",
    "            Inp.append(elem)\n",
    "\n",
    "        for n, elem  in enumerate(Out):\n",
    "            #print (n, elem)\n",
    "            if n < len(Out)//2:\n",
    "                elem.insert(0, 0)\n",
    "            else:\n",
    "                elem.insert(0, 1)\n",
    "\n",
    "        for n, elem in enumerate(Inp):\n",
    "            #print (n, elem)\n",
    "            if n < len(Inp)//2:\n",
    "                elem[0] = 1 - elem[0]\n",
    "            #print (n, elem)\n",
    "\n",
    "        #print(Out)\n",
    "        O = deepcopy(Out)\n",
    "        I = deepcopy(Inp)\n",
    "    \n",
    "    return Out, Inp \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = inverter_data(level = 5)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn generlized XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversion_depth = 10 #8, 9, 10\n",
    "\n",
    "neurons_start = 3   #9, 10, 10\n",
    "\n",
    "                    #120k, 400k, 800k\n",
    "    \n",
    "\n",
    "#n_hidden_layers = 22222 \n",
    "    \n",
    "\n",
    "n_step = 1\n",
    "\n",
    "#Xs = torch.tensor([[0.,0], [0,1],[1.,0], [1,1]]).cuda()\n",
    "#y = torch.tensor([[0.], [1], [1.], [0]]).cuda()\n",
    "\n",
    "Xs, y = inverter_data(inversion_depth)\n",
    "\n",
    "Xs[0][0] = float(Xs[0][0])\n",
    "y[0][0] = float(y[0][0])\n",
    "#print(Xs)\n",
    "#print(y)\n",
    "Xs = torch.tensor(Xs).cuda()\n",
    "y = torch.tensor(y).cuda()\n",
    "\n",
    "print(\"inversion_depth\", inversion_depth)\n",
    "\n",
    "h_nrs = []\n",
    "params = []\n",
    "epch = []\n",
    "\n",
    "for rep in range(10):\n",
    "\n",
    "    for i in range(neurons_start, 150, n_step):\n",
    "        hid = 1+i\n",
    "        loss, ep, cnv, prcnt, acc = train_once(inversion_depth, hid, Xs, y)\n",
    "\n",
    "        if cnv:\n",
    "            string = \"Converged with\"\n",
    "        else:\n",
    "            string = \"--- \"\n",
    "\n",
    "        print(string, \"loss:\", loss, \"in\", ep+1, \"epochs;\", hid, \"hidden neurons and\",  prcnt, \"parameters; Accuracy:\", acc,  \"; Repetition:\", rep+1)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if cnv:\n",
    "            break\n",
    "    \n",
    "    h_nrs.append(hid)\n",
    "    params.append(prcnt)\n",
    "    epch.append(ep+1)\n",
    "    \n",
    "print()\n",
    "print(\"inversion_depth\", inversion_depth)    \n",
    "print(h_nrs, np.mean(h_nrs), np.std(h_nrs))\n",
    "print(params, np.mean(params), np.std(params))\n",
    "print(epch, np.mean(epch), np.std(epch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data for AND and OR functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND_OR(depth = 2, logic = \"mix\"):\n",
    "    if depth == 1:\n",
    "        return 0, 0\n",
    "    \n",
    "    #if depth == 2: #AND\n",
    "    #    return [[0,0], [0,1], [1,0], [1,1]], [[0], [0], [0], [1]]\n",
    "    \n",
    "       \n",
    "    I = [[0,0], [0,1], [1,0], [1,1]]\n",
    "    \n",
    "    if logic == \"mix\" or logic == \"AND\":\n",
    "        O = [[0],[0],[0],[1]]\n",
    "        \n",
    "    else:\n",
    "        O = [[0],[1],[1],[1]]\n",
    "    #Out = deepcopy(O) \n",
    "    #Inp = deepcopy(I)\n",
    "    \n",
    "    if depth == 2:\n",
    "        return I, O\n",
    "    \n",
    "    for i in range(depth-2):\n",
    "        Out = deepcopy(O) \n",
    "        Inp = deepcopy(I)\n",
    "        \n",
    "        for elem in O:\n",
    "            Out.append(elem)\n",
    "\n",
    "        for elem in I:\n",
    "            Inp.append(elem)\n",
    "            \n",
    "            \n",
    "        for n, elem  in enumerate(Inp):\n",
    "            #print (n, elem)\n",
    "            if n < len(Out)//2:\n",
    "                elem.insert(0, 0)\n",
    "            else:\n",
    "                elem.insert(0, 1)\n",
    "\n",
    "        for n, elem in enumerate(Out):\n",
    "            #print (n, elem)\n",
    "            if n >= len(Inp)//2:\n",
    "                #elem[0] = 1 - elem[0]\n",
    "                #print (n, elem)\n",
    "        \n",
    "                if (i%2 == 0 and logic == \"mix\") or logic == \"OR\":\n",
    "                    #OR\n",
    "                    elem[0] = int(any(Inp[n][1:]))\n",
    "                    \n",
    "                else:\n",
    "                    #AND\n",
    "                    elem[0] = int(all(Inp[n][1:]))\n",
    "                \n",
    "        \n",
    "        \n",
    "        O = deepcopy(Out)\n",
    "        I = deepcopy(Inp)\n",
    "       \n",
    "         \n",
    "    return Inp, Out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = AND_OR(depth = 4, logic = \"AND\")\n",
    "print(X)\n",
    "print(\"---------------------------\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn AND and OR functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversion_depth = 12 #8, 9, 10\n",
    "\n",
    "neurons_start = 2   #9, 10, 10\n",
    "\n",
    "                    #120k, 400k, 800k\n",
    "    \n",
    "\n",
    "#n_hidden_layers = 22222 \n",
    "    \n",
    "\n",
    "n_step = 1\n",
    "\n",
    "#Xs = torch.tensor([[0.,0], [0,1],[1.,0], [1,1]]).cuda()\n",
    "#y = torch.tensor([[0.], [1], [1.], [0]]).cuda()\n",
    "\n",
    "#Xs, y = inverter_data2(inversion_depth)\n",
    "\n",
    "Xs, y = AND_OR(inversion_depth, \"AND\")\n",
    "\n",
    "Xs[0][0] = float(Xs[0][0])\n",
    "y[0][0] = float(y[0][0])\n",
    "#print(Xs)\n",
    "#print(y)\n",
    "Xs = torch.tensor(Xs).cuda()\n",
    "y = torch.tensor(y).cuda()\n",
    "\n",
    "print(\"inversion_depth\", inversion_depth)\n",
    "\n",
    "h_nrs = []\n",
    "params = []\n",
    "epch = []\n",
    "\n",
    "for rep in range(10):\n",
    "\n",
    "    for i in range(neurons_start, 150, n_step):\n",
    "        hid = 1+i\n",
    "        loss, ep, cnv, prcnt, acc = train_once(inversion_depth, hid, Xs, y)\n",
    "\n",
    "        if cnv:\n",
    "            string = \"Converged with\"\n",
    "        else:\n",
    "            string = \"--- \"\n",
    "\n",
    "        print(string, \"loss:\", loss, \"in\", ep+1, \"epochs;\", hid, \"hidden neurons and\",  prcnt, \"parameters; Accuracy:\", acc,  \"; Repetition:\", rep+1)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if cnv:\n",
    "            break\n",
    "    \n",
    "    h_nrs.append(hid)\n",
    "    params.append(prcnt)\n",
    "    epch.append(ep+1)\n",
    "    \n",
    "print()\n",
    "print(\"inversion_depth\", inversion_depth)    \n",
    "print(h_nrs, np.mean(h_nrs), np.std(h_nrs))\n",
    "print(params, np.mean(params), np.std(params))\n",
    "print(epch, np.mean(epch), np.std(epch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
